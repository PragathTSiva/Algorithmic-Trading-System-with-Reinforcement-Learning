{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ba3f08",
   "metadata": {},
   "source": [
    "# **RL AGENT QUICK‚ÄëSTART GUIDE**\n",
    "*Last updated: 2025-05-11*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5baaad",
   "metadata": {},
   "source": [
    "## **PART 0: OVERVIEW**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d9f27c",
   "metadata": {},
   "source": [
    "### **A) PURPOSE**\n",
    "Provide a **plug‚Äëand‚Äëplay** reinforcement‚Äëlearning stack‚Äîcallbacks, custom LSTM policy, trainer, and smoke‚Äëtest harness‚Äîfor experimenting with **PPO‚Äëbased trading agents** in any Gym‚Äëcompatible environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7648377",
   "metadata": {},
   "source": [
    "### **B) HOW TO USE THIS NOTEBOOK**\n",
    "1. Install dependencies (Part¬†1).\n",
    "2. Skim the package layout (Part¬†2) for available hooks.\n",
    "3. Implement your own Gym environment following the API contract (Part¬†3).\n",
    "4. Train & evaluate your agent with the example code cells (Part¬†4).\n",
    "5. Iterate‚Äîtune hyper‚Äëparameters, swap callbacks, or extend the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10081b96",
   "metadata": {},
   "source": [
    "## **PART 1: SETUP & DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd2f49",
   "metadata": {},
   "source": [
    "### **A) INSTALLATION**\n",
    "```bash\n",
    "pip install -r requirements.txt  # stable-baselines3, sb3-contrib, gymnasium, torch, pandas, etc.\n",
    "```\n",
    "üí° *For CUDA support, follow the official PyTorch instructions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5bda2a",
   "metadata": {},
   "source": [
    "## **PART 2: PACKAGE COMPONENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2557f",
   "metadata": {},
   "source": [
    "### **A) CALLBACKS & UTILITIES**\n",
    "| File | Highlights |\n",
    "| ---- | ---------- |\n",
    "| `callbacks.py` | `EarlyStoppingCallback` (reward‚Äëplateau detector)  \n",
    "`CheckpointCallback` (saves best model) |\n",
    "\n",
    "Both inherit SB3¬†`BaseCallback`, so they work with *any* SB3 / SB3‚ÄëContrib algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9593d6",
   "metadata": {},
   "source": [
    "### **B) CUSTOM POLICY**\n",
    "| File | Class | Architecture |\n",
    "| ---- | ----- | ------------ |\n",
    "| `policy.py` | `TradingLSTMPolicy` | 2‚Äëlayer 128‚Äëunit MLP ‚ûú 64‚Äëhidden LSTM ‚ûú actor & critic heads |\n",
    "\n",
    "Factory helper `make_trading_lstm_policy()` returns the class for easy SB3 registration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee566534",
   "metadata": {},
   "source": [
    "### **C) TRAINER & SMOKE TEST**\n",
    "* **Trainer** ‚Äì `train.py > train_agent()` wires everything (vectorised env, TensorBoard, callbacks) and persists `final_model.zip`.\n",
    "* **Smoke Test** ‚Äì `test_agent.py` spins up a minimal `SimpleTradingEnv`, runs a 20‚ÄØk‚Äëstep training loop, reloads the model, and makes a deterministic prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbaa13",
   "metadata": {},
   "source": [
    "## **PART 3: GYM ENVIRONMENT CONTRACT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d571c0",
   "metadata": {},
   "source": [
    "### **A) `reset()` & `step()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b4a4c",
   "metadata": {},
   "source": [
    "#### **Purpose**\n",
    "Cleanly initialise each episode and advance the environment **one step per agent action**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416023a",
   "metadata": {},
   "source": [
    "#### **Thought Process**\n",
    "A predictable, SB3‚Äëcompatible API lets the same agent implementation run on simulated order‚Äëbook data, synthetic price streams, or live feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3198896",
   "metadata": {},
   "source": [
    "#### **Method**\n",
    "```python\n",
    "def reset(self, seed=None, options=None):\n",
    "    self.pointer = 0        # time index\n",
    "    self.inventory = 0      # cleared position\n",
    "    return self._obs(), {}\n",
    "\n",
    "def step(self, action):\n",
    "    pnl   = self._execute(action)            # realised P&L if order fills\n",
    "    carry = -self.hold_cost_coeff * abs(self.inventory)\n",
    "    reward = pnl + carry\n",
    "\n",
    "    self.pointer += 1\n",
    "    terminated = self.pointer >= self.max_steps\n",
    "    info = {\n",
    "        'inventory': self.inventory,\n",
    "        'realized_pnl': pnl,\n",
    "        'action_mask': self._action_mask(),\n",
    "    }\n",
    "    return self._obs(), reward, terminated, False, info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b752c",
   "metadata": {},
   "source": [
    "### **B) ACTION MASKING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b488939",
   "metadata": {},
   "source": [
    "#### **Purpose**\n",
    "Prevent physically impossible moves‚Äîe.g. **selling with zero inventory** or **exceeding position limits**‚Äîfrom ever reaching the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d9d53",
   "metadata": {},
   "source": [
    "#### **Thought Process**\n",
    "Instead of adding a huge negative reward for invalid moves (which slows learning), we expose a Boolean *mask* so sampling & evaluation phases only consider **feasible actions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281b7e2e",
   "metadata": {},
   "source": [
    "#### **Method**\n",
    "* Return `info['action_mask']` on every `step()` call‚Äî`True` for valid indices, `False` otherwise.\n",
    "* Training helpers (`run_random_episode`, etc.) pick from the **masked set** when generating random actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ad5bf",
   "metadata": {},
   "source": [
    "### **C) REWARD FLOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4073a61",
   "metadata": {},
   "source": [
    "#### **Purpose**\n",
    "Align the learning signal with **portfolio objectives**‚Äîmaximise realised P&L while penalising risky inventory carry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff9eb0",
   "metadata": {},
   "source": [
    "#### **Thought Process**\n",
    "A simple additive scheme keeps the reward **scale stable** across assets and episodes, which helps PPO‚Äôs advantage normalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512287d5",
   "metadata": {},
   "source": [
    "#### **Method**\n",
    "```\n",
    "reward_t = realised_pnl_t  -  hold_cost_coeff * |inventory_t|\n",
    "```\n",
    "Where `realised_pnl_t` comes from trade executions and `hold_cost_coeff` is a tunable penalty for open positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b65ce8",
   "metadata": {},
   "source": [
    "## **PART 4: TRAIN & INFERENCE PLAYGROUND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ TRAIN ‚Äì 20k timesteps on the toy SimpleTradingEnv\n",
    "from agent.test_agent import SimpleTradingEnv\n",
    "from agent.train import train_agent\n",
    "\n",
    "env = SimpleTradingEnv(obs_dim=10, episode_length=100)\n",
    "\n",
    "config = dict(\n",
    "    total_timesteps=20_000,\n",
    "    n_envs=2,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=64,\n",
    "    n_steps=128,\n",
    "    early_stopping=True,\n",
    "    check_freq=1_000,\n",
    "    patience=3,\n",
    "    save_checkpoints=True,\n",
    "    save_freq=5_000,\n",
    "    use_custom_policy=True,\n",
    ")\n",
    "\n",
    "model = train_agent(env=env,\n",
    "                    config=config,\n",
    "                    log_dir='./logs/demo_run',\n",
    "                    save_path='./models/demo_run',\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç INFERENCE ‚Äì load and predict\n",
    "from sb3_contrib.ppo_recurrent import RecurrentPPO\n",
    "\n",
    "model = RecurrentPPO.load('./models/demo_run/final_model')\n",
    "\n",
    "env = SimpleTradingEnv(obs_dim=10, episode_length=100)\n",
    "obs, _ = env.reset()\n",
    "lstm_state = None\n",
    "\n",
    "action, lstm_state = model.predict(obs, state=lstm_state, deterministic=True)\n",
    "print(f'Predicted action: {action}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab573c84",
   "metadata": {},
   "source": [
    "## **PART 5: NEXT STEPS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba7a75",
   "metadata": {},
   "source": [
    "* Swap `SimpleTradingEnv` for your custom **LOBEnv**‚Äîthe trainer, callbacks, and policy require **zero changes**.\n",
    "* Point TensorBoard to `./logs` for live reward & loss curves.\n",
    "* Extend by:\n",
    "  * Writing new callbacks (e.g. Slack alerts)\n",
    "  * Replacing `RecurrentPPO` with `RecurrentA2C` or `RecurrentSAC` (one‚Äëliner change in `train.py`)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
