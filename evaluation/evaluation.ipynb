{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOcIeuQnlWL6Yl4KMWtsuEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **AGENT EVALUATION**"],"metadata":{"id":"UW7snuyBvxgb"}},{"cell_type":"markdown","source":["## **Part 1: Logging**\n","\n","The `Logger` class provides a streamlined way to record agent information, market states, actions, rewards, and any other relevant data. This data is output into a CSV file.\n","\n","#### **What does the Logger do?**\n","- Opens a CSV file at the specified path\n","- Infers or uses provided columns on first logging call\n","- Appends one row per `log_step()` call, recording necessary data\n","\n","#### **Why a Class?**\n","- Reusability: Having the `Logger` class allows us to instantiate multiple independent Loggers, which can give us separate logs for training, validation, and backtesting.\n","- Extensibility: Future additions such as supporting JSON output can be easily added by adapting the `Logger` class.\n","\n","#### **How to Use**\n","1. Import and create an instance of the Logger class before get the data to output:\n","  ```python\n","  from evaluation.logger import Logger\n","\n","  headers = [\n","      \"episode_id\", \"step_idx\", \"timestamp\", \"mid_price\",\n","      \"action\", \"reward\", \"pnl\", \"inventory\"\n","  ]\n","  logger = Logger(\"logs/agent_run.csv\", headers=headers)\n","  ```\n","2. Call `log_step()` at each step with arguments matching the headers:\n","  ```python\n","  for episode in range(num_episodes):\n","    for step in range(max_steps):\n","        # Run agent/environment update...\n","        logger.log_step(\n","            episode_id=episode,\n","            step_idx=step,\n","            timestamp=current_time,\n","            mid_price=env.mid_price,\n","            action=agent_action,\n","            reward=step_reward,\n","            pnl=agent.cumulative_pnl,\n","            inventory=agent.position\n","        )\n","  ```\n","3. Close the Logger to finish writing to the output file:\n","  ```python\n","  logger.close()\n","  ```"],"metadata":{"id":"oOoOqsMunkFD"}},{"cell_type":"markdown","source":["## **Part 2: Clustering and Agent Evaluation**\n","\n","In complex trading environments, an agent can exhibit a variety of behaviors -- sometimes switching between trend-following, mean-reversion, market-making or other unseen tactics as market conditions evolve. We use clustering and other data-driven evaluation methods to help visualize and understand the agent's actions and what strategies, existing or novel, the agent uses."],"metadata":{"id":"CAhpOTWOwbIO"}},{"cell_type":"markdown","source":["### **cluster_strategy_rediscovery**\n","\n","This function uses clustering to automatically rediscover any strategies that the agent may have used. Instead of assuming the agent only knows certain strategies by pre-defining them, we can discover which tactics the policy uses by visualizing the data. This is crucial for validating that the model has learned distinct behaviors and for monitoring how these behaviors emerge during training.\n","\n","#### **Usage**\n","\n","```python\n","from evaluation.cluster import cluster_strategy_rediscovery\n","\n","labels = cluster_strategy_rediscovery(df,\n","    pca_components=3,\n","    min_samples=10,\n","    min_cluster_size=50\n",")\n","df[\"strategy_cluster\"] = labels\n","```\n","\n","#### **How does this function work?**\n","- Reduces high-dimensional step-by-step logs into low-dimensional embedding using PCA.\n","- Runs HDBSCAN over the PCA embedding to create clusters that correspond to an existing strategy.\n","\n","#### **What features are used?**\n","- Normalized step index\n","- Inventory\n","- Market signals: spread, volume inbalance, price velocity\n","- Engineered features:\n","  - Price action correlation\n","  - Directional correctness\n","  - Churn - trades per step\n","  - 5-step smoothed velocity\n","  - Step-by-step inventory change\n","- Agent performance: reward, PnL\n","\n","#### **Why PCA + HDBSCAN?**\n","- PCA\n","  - Our raw feature set (inventory, spread, velocity, engineered features, etc.) exists in a high-dimensional space that manes it difficult to cluster.\n","  - PCA can reduce the dimensionality of this data by findign the axes that capture the most variance, preserving the key differences between strategies.\n","  - This technique speeds up clustering and reduces noise from redundant or low-variance features.\n","- HDBSCAN\n","  - We expect each core existing strategy to form its own \"cloud\" of similar behavior in PCA space, but these clouds may vary in density and size.\n","  - HDBSCAN automatically discovers clusters at multiple density levels, requires only a minimum cluster size (no É› to hand-tune), and explicitly labels outliers as noise.\n","  - This lets us reliably pull out each latent strategy without worrying about having to specify a single radius for all scenarios or merging distinct patterns."],"metadata":{"id":"f-vJAdztnPIp"}},{"cell_type":"markdown","source":["#### **HDBSCAN vs. DBSCAN**\n","\n","TODO"],"metadata":{"id":"E5rUpRCdg6UN"}},{"cell_type":"markdown","source":["### **cluster_novel_strategy**\n","\n","This function uncovers small, tight clusters of unusual or emerging behavior that aren't captured by the main strategy clusters -- these may be novel strategies. By exaggerating local neighborhoods and then using density-based clustering, it reveals small patterns or tactics that our agent explores only briefly. This is important for detecting agent behavior from unintended exploits to genuine strategies that could either enhance performance or introduce hidden risks.\n","\n","#### **Usage**\n","```python\n","  from evaluation.cluster import cluster_novel_strategy\n","\n","  novel_labels = cluster_novel_strategy(df,\n","      tsne_components=2,\n","      min_samples=10,\n","      min_cluster_size=50\n","  )\n","  df[\"novel_cluster\"] = novel_labels\n","```\n","\n","#### **How does this function work?**\n","- Runs t-SNE to project these high-dimensional features into a low-dimensional space that accentuates very local similarities, making tight and uncommon behavioral pockets stand out.\n","- Applies HDBSCAN on the t-SNE embedding to automatically discover dense micro-clusters and label other points as noise.\n","\n","#### **What features are used?**\n","- Reward\n","- Spread\n","- Volume imbalance\n","\n","#### **Why t-SNE + HDBSCAN?**\n","- t-SNE\n","  - Emphasizes very local similarities in the high-dimensional feature space, creating distinct islands from sparse micro-behaviors.\n","  - Ideal for revealing tight pockets of steps that traditional linear projetions would lump together.\n","- HDBSCAN\n","  - Automatically discovers clusters at all density levels, which allows us to catch both small, high-density novel tactics and larger strategies.\n","  - Explicitly labels low-density or transitional points as noise, keep the novel clusters focused on actual strategies."],"metadata":{"id":"E-GaYj7wQAH1"}},{"cell_type":"markdown","source":["### **evaluate_risk_awareness**\n","\n","This function measures per-episode risk and flags any data that surpasses the set thresholds as well as any statistical anomalies. By computing drawdowns, position volatility, and reaction lags, it quantifies how safely the agent trades under realistic constraints. Ensuring our model stays within predefined risk limits is essential when analyzing and potentially deploying emergent and rediscovered strategies.\n","\n","#### **Usage**\n","```python\n","  from evaluation.cluster import evaluate_risk_awareness\n","\n","  risk_df = evaluate_risk_awareness(df,\n","      drawdown_q=0.95,\n","      inv_std_q=0.95,\n","      lag_q=0.95,\n","      iforest_contam=0.05\n","  )\n","```\n","\n","#### **How does this function work?**\n","- Groups the log by `episode_id` and computes important features that determine the agent's risk level\n","- Sets quantile-based thresholds to classify episodes as \"safe\" vs. \"risky\"\n","- Runs an IsolationForest on these metrics to flag additional anomalies\n","\n","#### **What features are used?**\n","- `max_dd` - worst PnL drawdown\n","- `inv_std` - standard deviation of inventory\n","- `react_lag` - mean number of steps to respond the drawdown\n","\n","#### **Why thresholding + IsolationForest?**\n","- Thresholding\n","  - Provides clear, interpretable risk limits aligned with real-world tolerances.\n","  - Allows for a deterministic and reproducible system that ensures an episode either passes or fails in a predictable way.\n","- IsolationForest\n","  - Captures episodes that are unusual in the joint distributiojn of risk metrics, even if they don't surpass any single threshold.\n","  - Serves as a data-driven safety net for spotting complex risk patterns."],"metadata":{"id":"IYNj_hmMbcv-"}},{"cell_type":"markdown","source":["### **evaluate_profitability**\n","\n","This function assigns each episode to a profit category and tests whether the agent's profits are statistically significant. By bucketing episodes into low vs. high profit and running a one-sample t-test against zero, it provides both a performance breakdown and statistical confidence that returns are significant. Such statistical validation is key before claiming genuine trading success or comparing across different model versions.\n","\n","#### **Usage**\n","```python\n","  from evaluation.cluster import evaluate_profitability\n","\n","  profit_df, t_stat, p_val = evaluate_profitability(df)\n","```\n","\n","#### **How does this function work?**\n","- Aggregates per-episode metrics\n","- Buckets episodes into `low` vs. `high` profit based on PnL quantiles\n","- Performs a one-sample t-test of `end_pnl` against zero to assess statistical significance\n","\n","#### **What features are used?**\n","- `end_pnl` - total episode profit/loss\n","- `avg_reward` - average reward per step\n","- `trade_freq` - total trades in the episode\n","\n","#### *Why profit quantiles + one-sample t-test?**\n","- Profit quantiles\n","  - Offer a relative performance split which allows us to compare episodes on a consistent scale.\n","  - Reveals the distribution of returns, not just the mean.\n","- One-sample t-test\n","  - Provides a p-value and t-statistic that determines whether the mean profit is significantly different from zero or not.\n","  - Adds statistical rigor and confidence to profitability claims."],"metadata":{"id":"eJj7KG4KeaWu"}},{"cell_type":"markdown","source":["## **Part 3: Plotting**\n","\n","The `plot.py` file provides visualization functions for evaluating and understanding the performance of our trading agent. For each plot, we explain how it works and why it's useful.\n","\n","### **Plotting Functions**\n","- Reward per Episode\n","  - Groups the log by `episode_id` and sums the `reward` column for each episode.\n","  - Plots episode index against total reward using a simple line chart.\n","  - This function tracks the total reward the agent accumulates in each episode. This gives a high-level view of learning progress and stability over time, which is ideal for spotting trends, improvements, or plateaus in training.\n","- PnL Over Time\n","  - Groups the log by `episode_id` and for each episode, plots the `step_idx` against the `pnl` column.\n","  - This function visualizes the trajectory of realized profit and loss within each episode. It helps determine whether the agent is profiting, suffering big drawdowns, or fluctuating unpredictably.\n","- Action Frequency Distribution\n","  - Uses the `action` column and tallies up each action code.\n","  - Creates a bar chart of counts per action with readable labels mapped from their respective numeric codes.\n","  - This function examines how often the agent chooses each action (e.g., hold, buy, sell). Exposure to unbalanced distributions can indicate bias towards one action or insufficient exploration.\n","- Cluster Scatter\n","  - There two modes: one for plotting strategy rediscovery clustering and one for plotting novel strategy clustering.\n","  - Standardizes the data.\n","  - Runs PCA or t-SNE based on which mode was specified and calls the necessary clustering function to get the labels.\n","  - Plots each cluster in a scatter plot using these determined labels.\n","  - This function embeds high-dimensional agent data into two dimensions and applies clustering to discover patterns in strategy usage.\n","- Inventory Over Time\n","  - Groups by `episode_id` and plots `step_idx` by `inventory` for each episode.\n","  - This function keeps track of the agent's position throughout each episode. It is useful for verifying inventory limits, assessing risk exposure, and ensuring that holding penalties align with desired behavior.\n","- Risk Scatter Plot\n","  - Computes per-episode metrics using `evaluate_risk_awareness`.\n","  - Plots standard deviation of inventory against the max drawdown.\n","  - This function allows us to visualize the relationship between inventory volatility and maximum drawdown,. It helps evaluate if risk-aware components are effective.\n","- Anomaly Detection Histogram\n","  - Uses `evaluate_risk_awareness` to get an anomaly flag per episode.\n","  - Creates a bar chart of normal vs. anomaly counts.\n","  - This function shows the count of episodes that are anomalies, which are flagged by risk or behavior detectors. It is useful to quickly gauge how many runs fall outside normal strategies.\n","- Profitability by Quantile\n","  - Calls `evaluate_profitability` to get data grouped by `profit_quantile` and a one-sample t-test resule.\n","  - Creates a boxplot of `end_pnl`, distributed by quantile.\n","  - Compares end-of-episode PnL across quantiles, and stastically tests if the mean doesn't return zero. We can also use it to assess whether top-performming strategies significantly outperform or break-even."],"metadata":{"id":"6eWQBjUWwfBU"}}]}